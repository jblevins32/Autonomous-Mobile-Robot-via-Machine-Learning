{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning for path planning and control! This algorithm will simulate a gridmap and utilize reinforcement learning to determine the best path. The path will then be traversed with PID or LQR controllers in a ROS workspace on a Turtlebot3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit: https://ieeexplore.ieee.org/abstract/document/6573377"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from colorama import init, Fore, Back, Style\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Generate a simulation grid map to test the Q-learner and path planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a grid map class with random obstacles\n",
    "\n",
    "class GridGenerator:\n",
    "    def __init__(self, grid_size, num_obstacles):\n",
    "        self.grid_size = grid_size\n",
    "        self.num_obstacles = num_obstacles\n",
    "        self.grid = np.zeros((self.grid_size, self.grid_size), dtype=int)\n",
    "    \n",
    "    def generate_grid(self):\n",
    "        # Place obstacles randomly\n",
    "        obstacle_positions = random.sample(range(self.grid_size * self.grid_size), self.num_obstacles)\n",
    "        for pos in obstacle_positions:\n",
    "            row = pos // self.grid_size\n",
    "            col = pos % self.grid_size\n",
    "            self.grid[row, col] = -1  # Mark obstacle\n",
    "    \n",
    "    # Generate starting and goal states        \n",
    "    def generate_states(self):\n",
    "        states = random.sample(range(self.grid_size * self.grid_size), 2)\n",
    "        for state in states:\n",
    "            while self.grid[state // self.grid_size, state % self.grid_size] == -1:\n",
    "                state = random.sample(range(self.grid_size * self.grid_size), 1)\n",
    "        \n",
    "        # assign start and target to the plot\n",
    "        start_row = states[0] // self.grid_size\n",
    "        start_col = states[0] % self.grid_size\n",
    "        self.grid[start_row, start_col] = 1\n",
    "        \n",
    "        target_row = states[1] // self.grid_size\n",
    "        target_col = states[1] % self.grid_size\n",
    "        self.grid[target_row, target_col] = 2\n",
    "    \n",
    "    def print_grid(self):\n",
    "        for row in range(self.grid_size):\n",
    "            for col in range(self.grid_size):\n",
    "                if self.grid[row, col] == -1:\n",
    "                    print(\"██\", end=\" \")  # Print obstacle as black square\n",
    "                elif self.grid[row, col] == 1:\n",
    "                    print(\"S\", end=\" \")  # Print start as green square\n",
    "                elif self.grid[row, col] == 2:\n",
    "                    print(\"G\", end=\" \")  # Print goal as red square\n",
    "                else:\n",
    "                    print(\"  \", end=\" \")  # Print empty space\n",
    "            print()  # Move to the next line after each row\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the grid!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      ██    \n",
      "            ██       ██                               ██    \n",
      "██                                                          \n",
      "            ██                                              \n",
      "      ██                                  ██ ██             \n",
      "                                       ██                   \n",
      "                           ██ ██ ██                         \n",
      "   ██       ██                   ██          ██          ██ \n",
      "   S          ██             ██                         ██ \n",
      "██                                     ██                   \n",
      "                              ██                            \n",
      "██                      ██                                  \n",
      "            ██                                              \n",
      "                                             ██          ██ \n",
      "                                                G          \n",
      "         ██                               ██                \n",
      "            ██                ██    ██                      \n",
      "                           ██                         ██    \n",
      "                                          ██                \n",
      "      ██ ██                                        ██       \n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    grid_generator = GridGenerator(grid_size=20, num_obstacles=40)\n",
    "    grid_generator.generate_grid()\n",
    "    grid_generator.generate_states()\n",
    "    grid_generator.print_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build the q_learner class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_learner:\n",
    "    gamma = 1\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        \n",
    "    # Initialize all state and action pairs = 0 in a table. States = grid locations\n",
    "    def initialize(self, S, a):\n",
    "        table = np.zeros([S,a])\n",
    "        return table\n",
    "        \n",
    "    # Calculate reward function\n",
    "    def reward(self, S, a):\n",
    "        r = ...\n",
    "        return r\n",
    "    \n",
    "    # Observe the new state\n",
    "    def state_observation(self, S, a):\n",
    "        return S_new\n",
    "    \n",
    "    # Create Q value and update it in the graph\n",
    "    def update_entry(self, S, a, table):\n",
    "        r = self.reward(S, a)\n",
    "        S_new = self.state_observation(self, S, a)\n",
    "        \n",
    "        # Determine new Q value for this state and action\n",
    "        table(S,a) = r + self.gamma*np.max(table(S_new, a_))\n",
    "        \n",
    "        # Assign new state as the new current state\n",
    "        S = S_new\n",
    "        \n",
    "        return table, S\n",
    "        \n",
    "    # Run the algorithm\n",
    "    def run_q(self):\n",
    "        self.initialize()\n",
    "        # select an action and execute it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Q-learning is completed, we can implement it for our path planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class path_planner:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "    \n",
    "    # Run path planner    \n",
    "    def run_path(self):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
