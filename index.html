<!DOCTYPE html>
<html lang="en">
<head>
    <title>CS 7641 Project</title>
    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: sans-serif, Arial;
            /* margin: 1in; */
        }
        /* h1, h2, h3 {
            color: "w3-indigo";
        } */
        figure {
            text-align: center;
            margin: 20px 0;
            width: auto;
        }
        img {
            width: 100%;
        }
        figcaption {
            font-size: 0.9em;
        }
        table {
            width: auto;
            min-width: 75%;
            border-collapse: collapse;
            margin-bottom: 20px;
            margin-top: 0.75in;
            margin-left: auto;
            margin-right: auto;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        /* th {
            background-color: #f2f2f2;
        } */
        .heading {
            text-align: center;
        }
        #sec-intro, #sec-data, #sec-problem, #sec-methods, #sec-results, #sec-extras, footer {
            margin: 0.75in
        }
        footer div {
            margin-top: 10px;
        }
        /* #sec-extras {
            
        } */
    </style>
</head>
<body>

<!--From W3.CSS template-->
<div class="navbar">
    <div class="w3-bar w3-indigo w3-card w3-left-align w3-large">
        <a class="w3-bar-item w3-button w3-hide-medium w3-hide-large w3-right w3-padding-large w3-hover-white w3-large w3-red" href="javascript:void(0);" onclick="myFunction()" title="Toggle Navigation Menu"><i class="fa fa-bars"></i></a>
        <a href="#sec-intro" class="w3-bar-item w3-button w3-padding-large w3-hover-white">Introduction</a>
        <a href="#sec-data" class="w3-bar-item w3-button w3-hide-small w3-padding-large w3-hover-white">Dataset</a>
        <a href="#sec-problem" class="w3-bar-item w3-button w3-hide-small w3-padding-large w3-hover-white">Problem</a>
        <a href="#sec-methods" class="w3-bar-item w3-button w3-hide-small w3-padding-large w3-hover-white">Methods</a>
        <a href="#sec-results" class="w3-bar-item w3-button w3-hide-small w3-padding-large w3-hover-white">Results</a>
    </div>
</div>

<div class="heading">
    <h1>Autonomous Mobile Robot via Machine Learning</h1>
    <p>Jacob Blevins, Evan Boekweg, Ilias Baali, Anupama Nair, and Pierros-Christos Skafidas</p>
</div>

<div id="sec-intro">
    <h2 class="w3-text-indigo">Introduction/Background</h2>
    <p>Autonomous vehicles play a significant role in today's AI boom <a href="#hottopic">[1]</a>. From perception and localization to path planning and system control, the navigation stack for autonomous vehicles utilizes a multitude of complex algorithms <a href="#autonomous_review">[2]</a>. While widely-used classic algorithms exist for robots in deterministic environments, such as A* for path planning and linear quadratic regulators (LQR) for optimal control, machine learning (ML) algorithms are required for robots whose environments are highly stochastic or unknown. ML-based methods can provide improved results for the navigation stack when compared to their classical counterparts by assisting in constraint management, system analysis, disturbance management <a href="#control_CNN">[3]</a>, etc. ML algorithms are commonly applied in highly stochastic environments faced by autonomous vehicles such as Tesla <a href="#tesla">[4]</a> and Waymo <a href="#waymo">[5]</a> self-driving vehicles and many other robotic applications.</p>
        <p>The objective of this project is to develop ML-based algorithms for an autonomous, mobile robot that can navigate to an objective (defined as a care package) as in Figure 1. The chosen robot is a non-holonomic Turtlebot3; this robot utilizes two unilateral motors for motion control and includes LiDAR, encoder, and camera sensors. Similar projects for intelligent mobile robots have been performed in <a href="#autocontrol">[6]</a> and <a href="#mobilecontrol">[7]</a> where the authors utilize deep reinforcement learning to control vehicle motion; both experiments notice improved control efficiency in terms of time, energy, and stability.</p>
        <figure>
            <img src="figs/objective_fig.png" alt="Mobile Robot Objective" style="width:40%;">
            <figcaption>Figure 1: Mobile Robot Objective</figcaption>
        </figure>
</div>

<div id="sec-data">
    <h2 class="w3-text-indigo">Dataset</h2>
    <p>The Turtlebot's sensors are vital to the ML algorithm development as their data is utilized to train the ML models that determine the robot's actions. This data is recognized as follows:</p>
    <p>Datasets are collected by the Turtlebot's sensors:</p>
    <ul>
        <li>LiDAR (LDS-02 <a href="#lds02">[8]</a>)
            <ul>
                <li>Angle increment</li>
                <li>Distance measurements at each angle</li>
            </ul>
        </li>
        <li>Camera (Raspberry Pi Camera <a href="#camera">[9]</a>)
            <ul>
                <li>Video (1080p pixel image stream)</li>
            </ul>
        </li>
        <li>Wheel Encoder (XL430-W250 <a href="#encoders">[10]</a>)
            <ul>
                <li>Positions</li>
                <li>Velocities</li>
                <li>Orientations</li>
                <li>Angular velocities</li>
            </ul>
        </li>
    </ul>
</div>

<div id="sec-problem">
    <h2 class="w3-text-indigo">Problem Definition</h2>
    <p>For robots to make decisions in stochastic environments, the aforementioned operations require methods for understanding the random variables and conditions presented to them. In the case of this project, for perception/object detection, the target objective needs to be identified. In this case, the objective is a care package represented by an orange traffic cone; once the care package is detected, its location must be estimated with respect to the robot's body frame and marked as the goal. Next, the environment must be mapped, containing local obstacle information, and path planning should be generated from the starting location to the care package according to the mapped environment. Finally, system control should lead the mobile robot to the objective. These objectives are to be completed with a subset of navigation methods, utilizing ML-based algorithms where possible. For simplicity, this project is implemented in a ROS2 Gazebo simulation, but can be applied to a real world Turtlebot in the future.</p>
</div>
<div id="sec-methods">
    <h2 class="w3-text-indigo">Methods</h2>
    <h3>Implementation Framework</h3>
    <p>This project utilizes the Robot Operating System 2 (ROS2) on the Turtlebot's Ubuntu-OS Raspberry Pi SBU in order to run both classical and ML-based python scripts. These scripts, known as nodes in the ROS2 framework, communicate with each other over topics, services, and actions. Topics are one-way data streaming based, services are query based, and actions are for longer, feedback based communication. The framework for this project is demonstrated in Figure 1 where blue nodes represent the ML-based algorithms. Specifically, the YoloNode uses supervised learning to classify the care package, PosFinderNode uses unsupervised clustering to match the classification and 2D lidar data to a single instance of the care package, and the PpoNode uses reinforcement learning to send velocity commands to the ControllerNode. Further details of these ML-based algorithms are explained in Section 3.</p>

    <figure>
        <img src="figs/ROS_framework.png" alt="ROS2 Framework" style="width:100%;">
        <figcaption>Figure 1: ROS2 Framework</figcaption>
    </figure>

    <p>The nodes in Figure 1 begin to run when the system's launch file is executed. The Turtlebot sensors continuously publish data over topics to the appropriate subscriber nodes. These subscriber nodes subsequently process the data using various algorithms and pass it on to the respective next node. Ultimately, this sends velocity commands to the robot, causing it to navigate towards the care package. By using the ROS2 framework, this project can be easily reused and extended with other packages in the vast ROS2 ecosystem.</p>

    <p>The EncoderNode, LidarNode, and SlamNode are already implemented by the standard Turtlebot3 kit. It is the CameraNode, YoloNode, PosFinderNode, and PpoNode that are implemented for this project. The software architecture is described by the UML class diagram in Figure 2, which attempts to follow the Clean architecture style.</p>

    <figure>
        <img src="figs/uml_class_diagram.png" alt="UML class diagram representing the software architecture for this project. The dependency between layers points in only one direction, preventing cyclic dependencies. This architecture allows the top layer of software to be implemented without any knowledge of ROS2." style="width:100%;">
        <figcaption>Figure 2: UML class diagram representing the software architecture for this project. The dependency between layers points in only one direction, preventing cyclic dependencies. This architecture allows the top layer of software to be implemented without any knowledge of ROS2.</figcaption>
    </figure>
    
    <h3>Data Pre-Processing</h3>
    <p>The first step in ML implementation with messy sensor data is to clean the data, transforming it into useful information. From a robot simulation using Gazebo in ROS2, a simple room with 9 obstacles is generated, and a Turtlebot is controlled around the room. The LiDAR data capture, which has features listed in Section 1, is demonstrated in Figure 1 where the blue lines are LiDAR lasers.</p>

    <figure>
        <img src="figs/simulation.png" alt="Gazebo Simulation Environment" style="width:50%;">
        <figcaption>Figure 1: Gazebo Simulation Environment</figcaption>
    </figure>

    <p>The team must clean, combine, and pre-process the data for it to be usable. The LiDAR data consists of 360 range points, one for each degree around the vertical axis of the robot frame, and includes multiple ∞ values (invalid data). This data is replaced with <i>NaN</i> values, which is better handled by the utilized ML libraries. The raw data also consists of the list of distance measurements taken in a single frame under a single feature. To make it easier to read and utilize the measurements, each of the distances is separated and stored as one feature each. Since the frequency at which the LiDAR and encoder sensors record measurements differs, the two datasets are matched according to their closest timestamps and unnecessary data is deleted. Lastly, the LiDAR data is processed into a more usable format, that is, point clouds. The raw LiDAR data consists of angle and distance measurements taken in the sensor coordinate system. The measurements are first converted from polar coordinates to cartesian coordinates in the robot's frame. The odometry measurements are then used to shift the data into the world coordinate system. Finally, combining the measurements from all timestamps, the map of the environment is represented in Figure 2. While this map technically solves the mapping problem, this project utilizes an ML algorithm to piece together the map from the noisy real-world LiDAR and encoder data. This implementation tests the validity of unsupervised learning for mapping to be used in more complicated environments.</p>

    <figure>
        <img src="figs/lidar_map.png" alt="Lidar Map" style="width:50%;">
        <figcaption>Figure 2: Lidar Map</figcaption>
    </figure>

    <p>The LiDAR must be alternately pre-processed for the path planning and control implementation. Since the reinforcement learning algorithm utilized for these operations bases its reward functions on this LiDAR data, the ∞ values are replaced with the maximum allowable LiDAR range readings of 10. This data cleaning allows the networks to understand that there are no nearby obstacles in the direction of that data point. More explanation for this reasoning is given in Section 3.</p>

    <h3>ML Algorithms</h3>
    <p>The robot's mission can be split into three successive tasks, each associated with specific ML-based algorithms. All-in-all, 4 ML algorithms are implemented: 1 supervised, 2 unsupervised, and 1 reinforcement learning.</p>
        <ul>
            <li><strong>Object detection: Finding the Objective (YOLO).</strong> 
                In order for the Turtlebot to detect its target, computer vision perception techniques must be implemented. Through the use of the Turtlebot's camera, images of the environment are acquired at specific intervals to detect the presence of the care package--a traffic cone in this example--as well as its location. For that purpose, the team uses the YOLOv8 supervised learning algorithm. Unlike detection algorithms, such as CNN, R-CNN, and Faster R-CNN, this algorithm uses a single pass through a neural network for both the region proposal and the classification. This characteristic makes it faster and enables real-time application. In the context of autonomous vehicles, this quality is a critical requirement.
                <br><br>
                The YOLO algorithm is provided in the <em>Ultralytics</em> Python library. The library provides networks of different sizes, that can be trained using a specific dataset. For the project, the team decided to use the "nano" detection model, which is the smallest of the provided models. Having a smaller model reduces the prediction quality, but it allows faster predictions that can be used in real time. Considering the specific problem of detecting a single class of objects ("cone") and the relatively low importance of the spatial accuracy of the detection, the nano model is considered as the best option.
                <br><br>
                Considering the time constraints of the project, the model is trained using a pre-labeled traffic cones dataset. This data is obtained from a prebuilt set of cone images and is split into a training, validation, and a test dataset. The YOLO model is trained using this data with a limit of 100 epochs for the training step.
                <br><br>
                Once the cone is detected by the Turtlebot camera, frame transformations are to be utilized to teach the location of the care package with respect to the robot's body frame. This location is then sent to the path planning node. Results of training and testing this model are shown in Section 4.
            </li>
            <li><strong>Mapping: Creating the environment map (ICP).</strong> 
                As seen in Section 3, the measurements need to be processed and combined to form a map of the environment. The method used previously works well for clean data received from the simulation. Real-world data is often noisy, with the LiDAR data including a number of outliers and inaccuracies in encoder readings due to uneven terrain. Thus, ICP can be used to estimate the transform between different point cloud measurements with noisy data.
                <br><br>
                ICP (Iterative Closest Point) is an algorithm that connects point clouds from two different scans or timeframes to localize the robot and create a map of the environment. The steps in its implementation include:
                <ol>
                    <li>For each point in the first point cloud, match the closest point in the second point cloud. This is done using k-d trees to search for nearest neighbors.</li>
                    <li>Estimate the transform (rotation and translation) parameters that best aligns each point in the first cloud to its match in the second cloud. This is done by centering the points, and using SVD on the covariance matrix of the two point clouds to obtain the estimated rotation and translation parameters.</li>
                    <li>Transform the first point cloud using the transformation obtained above.</li>
                    <li>Iterate through the previous steps till the change in error is within a specified threshold, or it reaches a specified number of maximum iterations.</li>
                </ol>
            </li>
            <li><strong>Mapping: Understanding the environment (DBSCAN).</strong> 
                DBSCAN can be used to cluster obstacles together to help detect individual objects. This unsupervised learning method generates clusters based on the density of the LiDAR points. Pre-processing methods from Section 3 are utilized for data cleaning and then the following DBSCAN implementation is as follows:
                <ol>
                    <li>Find all neighbor points within eps and use these as core points if they have more than MinPts neighbors.</li>
                    <li>If that point isn't assigned to another cluster, then it's a new cluster.</li>
                    <li>Find all the density connected points and assign them to the cluster.</li>
                    <li>Iterate through the remaining points to assign them to clusters.</li>
                    <li>Finally, the points that are not in any cluster are classified as noise.</li>
                </ol>
            </li>
            <li><strong>Path Planning: Finding an optimal path to the objective (PPO).</strong> 
                Path planning and control execution is the final step of the navigation stack for the autonomous mobile robot. The implementation is as presented in research papers. The team utilizes the Proximal Policy Optimization (PPO), a deep reinforcement learning (RL) algorithm, to determine an optimal policy (network) that will direct the vehicle from start to the objective. Utilization of RL for path planning and control gives mobile vehicles the ability to navigate complex, stochastic environments without explicit programming. The specific use of PPO allows for a continuous (high-dimensional) action space required for continuously moving vehicles. Actor and critic deep neural network (DNN) models are fundamental to the PPO algorithm, wherein the actor defines the policy which gives the probability distribution of possible actions in a given state. On the other hand, the critic gives a value of each state which predicts the expected reward from each state. In parallel, as the actor and critic networks update, the robot improves its ability to choose optimal actions and evaluate potential rewards.
                <br><br>
                Initialization of the PPO framework begins with selecting the appropriate inputs to the algorithm. Feature engineering is utilized by reducing the LiDAR data to the 20 closest ranges. This ensures the LiDAR data does not overwhelm the other features. Additional inputs are 5 odometry data points: linear velocity, angular velocity, and robot x position, y position, and angle (v, ω, x, y, θ).
                <br><br>
                <strong>TRAINING:</strong> The training and testing/deployment algorithms are implemented using Pytorch and Python along with ROS2 and Gazebo. Ultimately, the objective is to learn the optimal policy by updating the actor DNN. In order to update the actor, the network is trained in episodes wherein the robot travels throughout the environment, attaining rewards and punishments along the way, finishing when a time limit has completed. Back propagation (updating of network weights) within training is completed by evaluating the loss as a function of policy loss and value loss:
                <br><br>
                <pre>
                <code>
L = L_policy + L_value
L_policy = E[min(rA, clip(r, 1-ε, 1+ε))]
L_value = 0.5E[A^2]
                </code>
                </pre>
                <br>
                where probability ratio: r = log_prob / log_prob_old = exp(log_prob - log_prob_old) to determine how much the new policy deviates from the previous episode's or "old" policy. The clip function keeps r within the bounds of (1-ε, 1+ε) where ε is a clipping hyper-parameter to determine the allowed rate of policy changes. A = R_d - SV is the advantage function. R_d is the discounted reward and SV is the state value, determined from the reward of the actor output and the output of the critic network. Rewards are discounted in order to favor those that are more immediate since the present state is more important than the future.
                <br><br>
                The actor is given the 25 inputs and returns a mean and standard deviation for v and ω each, i.e. μ_v, μ_ω, σ_v, and σ_ω. These parameters represent Gaussian distributions which are sampled in training to determine the actions for the robot to take. Sampling is a stochastic policy method utilized instead of simply taking the mean actions which provides a balance of exploration and exploitation (not using the mean versus using the mean). As a result, the diversified actions prevent the policy from remaining in local minima and aid it in handling uncertainty. These actions, v and ω, represent the control commands which are sent to the actuators. Training takes place in the following steps for a given maximum number of steps in each episode.
                <ol>
                    <li>Generate action mean and standard deviation from the actor network based on state input.</li>
                    <li>Send action (commands) to the actuators.</li>
                    <li>Add states, actions, and rewards as information to a buffer.</li>
                    <li>Get new sensor values (states).</li>
                </ol>
                After the maximum steps have passed, within the same episode, the policy is updated according to the loss defined in the equation above and the next training episode begins. Rewards from each action implementation are as follows:
                <br><br>
                <pre>
                <code>
R = R_collision + R_goal + R_non-stationary + R_flip
R_collision = - lr_collision / e^(mean(obstacle distance)) → Punishment for being closer to obstacles
R_goal = lr_goal / e^(mean(goal distance)) → Reward being closer to the goal
R_non-stationary = v → Reward being non-stationary
R_flip = - lr_flip if roll or pitch > ρ → Punishment for flipping over
                </code>
                </pre>
                <br>
                where lr is learning rates for various rewards and punishments. The exponentials are taken in order to greater punish or reward as the robot gets closer to obstacles or the goal, respectively; otherwise, these rewards and punishments provide too much feedback further from the obstacles or objectives, making the robot afraid to even move from its starting position. These functions are shown in the figure below where the goal function is set to give a higher reward with respect to the obstacle punishment in order to greater promote getting to the objective.
                <br><br>
                <img src="figs/reward_function.png" alt="Goal and Collision Reward Functions" style="width:100%;">
                <br><br>
                Policy updating is the core of any RL algorithm. The policy update steps are as follows which occur after each training episode and the buffer is filled with information. This loop occurs for a set number of training epochs:
                <ol>
                    <li>Unpack buffer contents (states, actions, rewards).</li>
                    <li>Generate discounted reward R_d from each reward in the buffer.</li>
                    <li>Generate action mean, standard deviation, and state values SV based on states from the actor network.</li>
                    <li>Calculate losses according to the equation above.</li>
                    <li>Compute gradients for the networks.</li>
                    <li>Update network weights.</li>
                </ol>
                The weight update utilizes the Adaptive Moment Estimation (Adam) optimizer which is similar to stochastic gradient descent. Once the training is completed for all episodes, the model is saved and ready for deployment. During deployment, the actor network receives the same inputs as in training and calculates only a forward propagation and sends the mean of the action distributions to the actuators.
            </li>
        </ul>
</div>

<div id="sec-results">
    <h2 id="sec-results">Results and Discussion</h2>
        <p>The goal of this project is to autonomously navigate a mobile robot to an objective with ML model scoring metrics as in Section <a href="#subsec-scoring">3.2</a>. The robot maps its environment, locates itself within the environment, identifies its target, and then attempts to move to its target while avoiding collisions. However, the PPO reinforcement learning algorithm converged on a non-ideal solution, so a future step is to improve this step in the project pipeline.</p>

        <h3 id="subsec-scoring">ML Model Scoring, Results, and Discussion</h3>
        <ul>
            <li><strong>Object detection algorithm (YOLO).</strong>
                <p>The YOLOv8 model is trained on the training dataset, with a limit set to 100 epochs. The convergence is evaluated based on two loss metrics: one related to the ability of the model to accurately detect the object on the image and draw the corresponding box (<em>box_loss</em>), and another related to the classification capability (<em>cls_loss</em>). The box loss is calculated based on the CIoU loss function, that accounts for the overlap between the predicted actual boxes, but also the difference in aspect ratio and the distance between centers, while the classification loss is based on cross-entropy. These two metrics were tracked for the training and validation datasets, and are plotted on Figure 1. As one can see, the validation losses were still slowly decreasing at the end of the training, which suggests that the model was still improving and would have likely benefited from a higher number of epochs. However, the small slopes also suggests that the difference would not have been major and that this final model should be a good compromise between training duration and performance. This is confirmed by the high precision, recall, and mAP50 highlighted in Figure 1. For that specific problem, the mAP50 corresponds to the area under the precision-recall curve calculated at an intersection over union (IoU) threshold of 0.50. The IoU corresponds to the ratio between the overlap of the predicted box and ground truth box, and the area of the union of these two boxes. The high value of these metrics, around 0.85-0.9, shows that the trained model performs well on the validation set.</p>

                <p>Each prediction of the model is associated with the probability that the object is a cone. Depending on the detection threshold used on this probability to accept or reject the detection, the performance of the model changes, as can be seen on Figure 2 which shows the evolution of the model's precision, recall, and F1 score for different confidence thresholds. With no surprise, the highest precision is achieved using a higher confidence threshold, since keeping the prediction with only high probability of being cones reduces the risk of false positive. In the mean time a higher threshold also reduces the recall, since fewer cones are correctly classified due to their probability being lower than the threshold. In order to find the best compromise, one can use the F1 score that combines both metrics, and find the threshold value that maximizes it. In this specific case, the optimum confidence threshold seems to be 0.467. The corresponding precision is 0.88, and the recall is 0.82. On the precision-recall curve, this point corresponds to the top right corner of the curve, giving the optimal compromise (Figure 3).</p>

                <p>Once the model created and validated, it was tested to detect a cone on a few custom pictures to assess its performance and limitations. Overall, the detection works great in cases where the cone is mostly visible, with the model giving high prediction probabilities. However, the model starts to show some limitations when it comes to cases for which most of the cone is obstructed. This issue likely stems from the training dataset which mainly contains isolated cones. Better results could likely be obtained by creating a custom dataset with partially hidden cones, and training the algorithm on this data. However, with the time limitations of this project in mind, the model performs sufficiently well for cone detection. Several examples are shown in Figure 4, and a working example within the ROS pipeline is shown in Figure 5.</p>

                <div class="figure">
                    <img src="figs/yolo_metrics/results.png" alt="Evaluation metrics evolution across training">
                    <p>Figure 1: Evaluation metrics evolution across training. The x axis is labeled in epochs.</p>
                </div>

                <div class="figure">
                    <img src="figs/yolo_metrics/PR_curve.png" alt="Precision-Recall curve">
                    <p>Figure 2: Precision-Recall curve. The optimal point on the top right corner corresponds to a confidence threshold of 0.467.</p>
                </div>

                <div class="figure">
                    <img src="figs/yolo_metrics/P_curve.png" alt="Precision curve">
                    <p>Figure 3a: Precision curve</p>
                </div>
                <div class="figure">
                    <img src="figs/yolo_metrics/R_curve.png" alt="Recall curve">
                    <p>Figure 3b: Recall curve</p>
                </div>
                <div class="figure">
                    <img src="figs/yolo_metrics/F1_curve.png" alt="F1 curve">
                    <p>Figure 3c: F1 curve</p>
                </div>
                <div class="figure">
                    <p>Figure 3: Precision, recall, and F1 score for different confidence thresholds.</p>
                </div>

                <div class="figure">
                    <img src="figs/conde_detection.png" alt="Examples of detection tests">
                    <p>Figure 4: Examples of detection tests. On the top, the cone is mostly visible and the model is able to detect it with high probability. On the bottom left, the cone was mostly hidden but was still detected with a probability of 0.5. On the bottom right, the model was unable to detect the cone.</p>
                </div>

                <div class="figure">
                    <img src="figs/yolo_cone_detection_sim.png" alt="Cone detection implemented in the ROS pipeline and simulated in Gazebo">
                    <p>Figure 5: Cone detection implemented in the ROS pipeline and simulated in Gazebo.</p>
                </div>
            </li>

            <li><strong>Mapping Algorithm (ICP).</strong>
                <p>Iterative Closest Point is implemented to help improve the performance of the mapping process in more noisy environments. For example, ICP was used to match the point clouds at timeframes 0 and 150 here. As we can see, it transforms the point cloud almost correctly to the frame of the point cloud at timeframe 0.</p>

                <div class="figure">
                    <img src="figs/icp_map.png" alt="Sample ICP between timeframes 0 and 150">
                    <p>Figure 6: Sample ICP between timeframes 0 and 150.</p>
                </div>

                <p>To compare the two methods, RMSE values were calculated between the two matched point clouds. The LiDAR processing using encoders as seen in Section <a href="#sec-preprocessing">2</a> yielded a RMSE value of 0.228, whereas the ICP method gave 0.232. As seen from both the map as well as the RMSE values, the ICP algorithm produced more error than using only encoder readings.</p>

                <p>The reasons for the above could include the following:</p>
                <ul>
                    <li>The data currently being used is obtained from a simulation, where the LiDAR and encoders work without error. This would cause the map formed by using encoder readings to be more accurate than estimating transforms using ICP.</li>
                    <li>Point clouds in each timeframe contain new points from LiDAR measurements, thus matching two different point clouds completely will not be possible, thus producing error in the estimated transformation matrix.</li>
                </ul>
            </li>

            <li><strong>Mapping Algorithm DBSCAN.</strong>
                <p>DBSCAN can be used to cluster obstacles together to help us detect them in the environment and filter noise. As a sample test, we used our DBSCAN implementation with Eps, MinPts set to 0.7 and 3, respectively. In Figure 7, we see that the obstacles and the walls were classified correctly.</p>

                <p>The next step in our implementation is to experiment with multiple model parameters:</p>
                <ul>
                    <li>Eps: 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0</li>
                    <li>MinPts: 3,4,5,6,7,8,9,10</li>
                </ul>
                <p>To compare the cluster quality, we used the Silhouette Coefficient score. After testing out the different parameter pairs, the best pair was eps=0.1, and MinPts=3 with a Silhouette Coefficient score of -0.21215831709613261. In Figure 8, we see the optimized clustering result.</p>

                <div class="figure">
                    <img src="figs/DBSCAN_e07_n3.png" alt="Sample DBSCAN Clusters (eps=0.7 MinPts=3)">
                    <p>Figure 7: Sample DBSCAN Clusters (eps=0.7 MinPts=3).</p>
                </div>

                <div class="figure">
                    <img src="figs/optimal_DBSCAN_clustering.png" alt="Optimized DBSCAN Clusters (eps=0.7 MinPts=3)">
                    <p>Figure 8: Optimized DBSCAN Clusters (eps=0.1 MinPts=3).</p>
                </div>
            </li>

            <li><strong>Path Planning (PPO).</strong>
                <p>The PPO algorithm is trained over 1000 episodes with each episode lasting 10,000 time steps. Each training occurs over 10 epochs for each episode. Learning rates for the rewards are set as $lr_{collision} = 10, lr_{goal} = 50, and lr_{flip} = 5$. The rewards from training are as shown in Figure 9.</p>

                <div class="figure">
                    <img src="figs/reward_plot.png" alt="PPO Training Rewards per Episode">
                    <p>Figure 9: PPO Training Rewards per Episode.</p>
                </div>

                <p>Typically, a transient period should be visible in the rewards before they settle at a steady state value. As seen in the image, the training did not replicate this desired behavior and instead learned over time that spinning in place at the starting location is the best method for optimizing its reward. As a result, the PPO algorithm did not perform as desired, but future tuning of rewards and loss calculations could provide better results. Due to the extensive training times, additional tests of tuned parameters is not viable for this project. When the non-optimal actor policy is deployed to the robot, it somewhat navigates, but still hits obstacles, tipping it over. A few training episodes reached the goal as indicated by the spikes in the reward figure, but not enough to train the model to perform that set of actions consistently.</p>
            </li>
        </ul>

        <h3>Next Steps</h3>
        <p>The algorithms were not able to be tested on a physical robot or completely optimized due to time constraints. For this reason, next steps include:</p>
        <ul>
            <li>Build physical test environment</li>
            <li>Tune and optimize algorithms further</li>
            <li>Knit together algorithms and ROS2 workspace</li>
            <li>Execute workspace on physical Turtlebot and assess its performance</li>
        </ul>
        <div class="figure">
            <img src="figs/gantt.png" alt="Team Gantt Chart">
            <p>Figure 10: Team Gantt Chart</p>
        </div>
        <table class="table">
            <caption>Table 1: Team Contributions</caption>
            <thead>
                <tr>
                    <th>Name</th>
                    <th>Final Report Contributions</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Jacob Blevins</td>
                    <td>Literature review, problem definition, ROS2 implementation, and PPO algorithm</td>
                </tr>
                <tr>
                    <td>Evan Boekweg</td>
                    <td>ROS2 implementation, LiDAR raw data collection & extraction, and camera implementation</td>
                </tr>
                <tr>
                    <td>Ilias Baali</td>
                    <td>YOLO algorithm and computer vision</td>
                </tr>
                <tr>
                    <td>Anupama Nair</td>
                    <td>LiDAR data pre-processing and KNN</td>
                </tr>
                <tr>
                    <td>Pierros-Christos Skafidas</td>
                    <td>Dataset review, LiDAR data pre-processing, and DBSCAN</td>
                </tr>
            </tbody>
        </table>
</div>

<footer class="w3-container w3-padding-64 w3-opacity">
    <div>
        <div id="autocontrol" class="csl-entry">
            [1] Xia, W., Li, H., & Li, B. (2016). A Control Strategy of Autonomous Vehicles Based on Deep Reinforcement Learning. <i>2016 9th International Symposium on Computational Intelligence and Design (ISCID)</i>, <i>2</i>, 198-201. https://doi.org/10.1109/ISCID.2016.2054
        </div>
    
        <div id="mobilecontrol" class="csl-entry">
            [2] Ruan, X., Ren, D., Zhu, X., & Huang, J. (2019). Mobile Robot Navigation based on Deep Reinforcement Learning. <i>2019 Chinese Control And Decision Conference (CCDC)</i>, 6174-6178. https://doi.org/10.1109/CCDC.2019.8832393
        </div>
    
        <div id="hottopic" class="csl-entry">
            [3] Cann, D. O. (2016). These are the top 10 emerging technologies of 2016. <i>Media Relations World Economic Forum</i>.
        </div>
    
        <div id="waymo_data" class="csl-entry">
            [4] Waymo. (2024). Waymo Open Dataset. Accessed June 11, 2024. <a href="https://waymo.com/open">https://waymo.com/open</a>.
        </div>
    
        <div id="waymo" class="csl-entry">
            [5] Waymo. (2024). Waymo. Accessed June 11, 2024. <a href="https://waymo.com">https://waymo.com</a>.
        </div>
    
        <div id="tesla" class="csl-entry">
            [6] Tesla. (2024). Tesla. Accessed June 11, 2024. <a href="https://tesla.com">https://tesla.com</a>.
        </div>
    
        <div id="lds02" class="csl-entry">
            [7] LDS-02. (2024). Accessed June 11, 2024. <a href="https://emanual.robotis.com/docs/en/platform/turtlebot3/appendix_lds_02/">https://emanual.robotis.com/docs/en/platform/turtlebot3/appendix_lds_02/</a>.
        </div>
    
        <div id="camera" class="csl-entry">
            [8] Raspberry Pi Camera. (2024). Accessed June 11, 2024. <a href="https://emanual.robotis.com/docs/en/platform/turtlebot3/appendix_raspi_cam/">https://emanual.robotis.com/docs/en/platform/turtlebot3/appendix_raspi_cam/</a>.
        </div>
    
        <div id="encoders" class="csl-entry">
            [9] Encoder. (2024). Accessed June 11, 2024. <a href="https://emanual.robotis.com/docs/en/platform/turtlebot3/appendixes/#more-info">https://emanual.robotis.com/docs/en/platform/turtlebot3/appendixes/#more-info</a>.
        </div>
    
        <div id="silhouette_score" class="csl-entry">
            [10] SciKit-Learn Developers. (2024). Silhouette Score. Accessed June 14, 2024. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html</a>.
        </div>
    
        <div id="KalmanFilter1D" class="csl-entry">
            [11] Wilson, D. (2024). The Kalman Filter: An Example of Using 1-D Filters. Accessed June 14, 2024. <a href="https://www.kalmanfilter.net/kalman1d.html">https://www.kalmanfilter.net/kalman1d.html</a>.
        </div>
    
        <div id="yolo" class="csl-entry">
            [12] Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You Only Look Once: Unified, Real-Time Object Detection. <i>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 779-788. https://doi.org/10.1109/CVPR.2016.91
        </div>
    
        <div id="Q-Learning" class="csl-entry">
            [13] Konar, A., Goswami Chakraborty, I., Singh, S. J., Jain, L. C., & Nagar, A. K. (2013). A Deterministic Improved Q-Learning for Path Planning of a Mobile Robot. <i>IEEE Transactions on Systems, Man, and Cybernetics: Systems</i>, <i>43</i>(5), 1141-1153. https://doi.org/10.1109/TSMCA.2012.2227719
        </div>
    
        <div id="traffic-cones-4laxg_dataset" class="csl-entry">
            [14] robotica. (2022). Traffic Cones Dataset. Roboflow Universe. Accessed July 3, 2024. <a href="https://universe.roboflow.com/robotica-xftin/traffic-cones-4laxg">https://universe.roboflow.com/robotica-xftin/traffic-cones-4laxg</a>.
        </div>
    
        <div id="UltralyticsYOLO2024" class="csl-entry">
            [15] Ultralytics. (2024). Ultralytics YOLO Documentation: Licenses. Accessed July 3, 2024. <a href="https://docs.ultralytics.com/#yolo-licenses-how-is-ultralytics-yolo-licensed">https://docs.ultralytics.com/#yolo-licenses-how-is-ultralytics-yolo-licensed</a>.
        </div>
    
        <div id="lidar_path_planning" class="csl-entry">
            [16] Zamanakos, G., Tsochatzidis, L., Amanatiadis, A., & Pratikakis, I. (2021). A comprehensive survey of LIDAR-based 3D object detection methods with deep learning for autonomous driving. <i>Computers & Graphics</i>, <i>99</i>, 153-181. https://doi.org/10.1016/j.cag.2021.07.003. URL: <a href="https://www.sciencedirect.com/science/article/pii/S0097849321001321">https://www.sciencedirect.com/science/article/pii/S0097849321001321</a>.
        </div>
    
        <div id="control_CNN" class="csl-entry">
            [17] Spielberg, N. A., Brown, M., Kapania, N. R., Kegelman, J. C., & Gerdes, J. C. (2019). Neural network vehicle models for high-performance automated driving. <i>Science Robotics</i>, <i>4</i>(28), eaaw1975. https://doi.org/10.1126/scirobotics.aaw1975. URL: <a href="https://www.science.org/doi/abs/10.1126/scirobotics.aaw1975">https://www.science.org/doi/abs/10.1126/scirobotics.aaw1975</a>. Eprint: <a href="https://www.science.org/doi/pdf/10.1126/scirobotics.aaw1975">https://www.science.org/doi/pdf/10.1126/scirobotics.aaw1975</a>.
        </div>
    
        <div id="PPO_algo" class="csl-entry">
            [18] Jin, X., & Wang, Z. (2022). Proximal policy optimization based dynamic path planning algorithm for mobile robots. <i>Electronics Letters</i>, <i>58</i>(1), 13-15. https://doi.org/10.1049/ell2.12342. URL: <a href="https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/ell2.12342">https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/ell2.12342</a>. Eprint: <a href="https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/ell2.12342">https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/ell2.12342</a>.
        </div>
    
        <div id="PPO_algo2" class="csl-entry">
            [19] Taheri, H., & Hosseini, S. (2024). Deep Reinforcement Learning with Enhanced PPO for Safe Mobile Robot Navigation. URL: <a href="https://arxiv.org/html/2405.16266v1">https://arxiv.org/html/2405.16266v1</a>.
        </div>
    
        <div id="autonomous_robot" class="csl-entry">
            [20] Wang, X. (2024). Autonomous Mobile Robot Visual SL. Accessed June 11, 2024.
        </div>
    </div>
</footer>

</body>
</html>